We propose to combine parse forest and discourse structures to form a unified representation for a paragraph of text.   The purpose of this representation is to tackle answering complex paragraphsized questions in a number of products and servicesrelated domains. A candidate set of answers, obtained by a keyword search, is reranked by matching the sequence of parse trees of an answer with that of the question. To do that, a graph representation and learning technique for parse structures for paragraphs of text have been developed. Parse Thicket PT as a set of syntactic parse trees augmented by a number of arcs for intersentence wordword relations such as coreference and taxonomic relations is introduced. These arcs are also derived from other sources, including Speech Act and Rhetoric Structure theories. The operation of generalization of logical formulas is extended towards parse trees and then towards parse thickets to compute similarity between texts. 
   We provide a detailed illustration of how PTs are built from parse trees, and generalized. The proposed approach is subject to evaluation in the product search and recommendation domain of eBay.com, where user queries include product names, desired features and expressions for user needs in multiple sentences. We demonstrate that search relevance is improved by PT generalization, using Bing search engine API as a baseline. We perform the comparative analysis of contribution of various sources of discourse information to the relevance. An open source plugin for SOLR is developed so that the proposed technology can be easily integrated with industrial search engines.
Parse trees were found fairly useful in solving text classification and text relevance problems, including search. Parse trees have become a standard form of representing the syntactic structures of sentences Abney,  Punyakanok et al., . A number of approaches to learning parse trees have been proposed, including convolution kernel based on support vector SVM learning Collins and Duffy,  Haussler,   Moschitti, , and structural similarity based on direct matching of parse trees for sentences Galitsky et al . 
    As to the structure of a paragraph of text, there is no generally accepted model. Such a model needs to rely on the parse forest for the sentences this paragraph comprises, and also need to include a paragraphlevel discourse information.  HowIn this study we will attempt to represent a linguistic structure of a paragraph of text based on parse trees for each sentence of this paragraph. We will refer to the sequence of parse trees plus a number of arcs for intersentence relations of the discourse type between the nodes for words as Parse Thicket PT. A PT is a graph, which includes parse trees for each sentence, as well as additional arcs for intersentence relationship. Parse thickets are inspired by the problem of answering complex paragraphsize questions, which needs to be matched with paragraphs of candidate answers.
    We explore a number of sources for the links between words in a paragraph other than syntactic. Our sources are coreferences, entityentity and other taxonomic relations, discourse relations such as rhetoric relations between elementary discourse units, and speech act relations. For each of these sources we determine whether it can be leveraged by more accurate assessment of similarity between paragraphs of text. The main goal here is to make similarity measure independent of how text / phrases is distributed through sentences.
    A vast majority of linguistic theories, from cognitive to functional theories of grammars, rely on need some form of representation of structured data for natural language processing Lamberti et al . Once a linguistic representation becomes richer than the bagofwords, there is a need for a systematic way to compare such representations. However, there is a lack of formal models to compare linguistic structures beyond parse trees for individual sentences. In this study we introduce PTs as a structural machine learning framework to operate with multiple syntactic parse trees for sentences in paragraphs of text. 
Nowadays, commercial search engines are not very good at tackling paragraphlevel queries consisting of multiple sentences. They either find very similar documents, if they are available, or very dissimilar ones, so that search results are not very useful to the user. This is due to the fact that for multisentences queries it is rather hard to learn ranking based on user clicks, since the number of longer queries is practically unlimited. Hence we need a linguistic technology, which would rank candidate answers based on structural similarity between the question and the answer. In this study we build a graphbased representation for a paragraph of text so that we can track the structural difference between these paragraphs, taking into account not only parse trees, but the whole discourse of both the question and answers 
The demand for access to different types of information have led to a renewed interest in answering questions posed in ordinary human language and seeking  exact, specific and complete answer. After having made substantial achievements in factfinding and list questions, natural language processing NLP community turned their attention to more complex information needs that cannot be answered by simply extracting named entities persons, organization, locations, dates, etc. from single sentences in documents Chali et al . Unlike simple factoid questions, complex questions often seek multiple different types of information simultaneously, located in multiple sentences, and one cannot assume that a particular single sentence contains expected information. Dependency parsing helps to answer complex queries in an industrial environment, reconstructing semantic content efficiently by extracting related terms through analysis and classification Iwashita et al . To systematically analyze how keywords from a query occur in multiple sentences in a document, one needs to explore coreferences and other relations between words within a sentence and between sentences.
  Most search engines attempt to find the occurrence of query keywords in a single sentence in a candidate search result Kim et al . If it is not possible or has a low search engine score, multiple sentences within one document are used. However, modern search engines have no means to determine if the found occurrences of the query keywords in multiple sentences are related to each other or not. Neither search engine can determine if they are related to the same entity, nor, being in different sentences, are all related to the query term.
Paragraphs of text as queries appear in the searchbased recommendation domains Montaner et al.,  Bhasker and Srikumar  Thorsten, . Recommendation agents track user chats, user postings on blogs and forums, user comments on shopping sites, and suggest web documents and their snippets, relevant to a purchase decisions. To do that, these recommendation agents need to take portions of text, produce a search engine query, run it against a search engine API such as Bing or Yahoo, and filter out the search results which are determined to be irrelevant to a purchase decision. The last step is critical for a sensible functionality of a recommendation agent, and poor relevance would lead to a lost trust in the recommendation engine. Hence an accurate assessment of similarity between two portions of text is critical to a successful use of recommendation agents.
Nowadays, the problem of answering simple queries, involving a single entity and its attributes, is solved fairly well. However, more complex questions in such domain as legal, science, and health can be expressed in paragraphs rather than in single phrases or single sentences, as expressed, for example, in Yahoo Answers or StackOverflow. The technique being proposed targets the paragraphsized questions which involve multiple entities and their interconnected attributes. These attributes do not occur altogether but instead follow a certain discourse, which needs to be extracted from the question and then matched with the ones from candidate answers. For example, to answer the following question, we do not just need to match the keywords from actual questions two last sentences but also match the preceding sentences with that of a candidate answer.
One of the provisions in the Patient Protection and Affordable Care Act  is that it limits the profits of health insurance companies. The ACA imposes a minimum medical loss ratio MLR on all insurers. The MLR is the amount of money spent on covered person medical care divided by the total revenue received through premiums. What constitutes ‘medical care’? Do investments in electronic health records count as medical care? 
If a question includes just a phrase or a sentence, selecting less frequent keywords to match with candidate answers do the job. However, in this case one needs to track how entities like ‘medical care’ are introduced and how the links between their attributes are established. To match this paragraphsized question with an answer, one needs to build a representation of its discourse on one hand PT, and also provide a machinery to match this discourse structure with that of a candidate answer PT generalization. 
Fig. A high level view of the contribution of this paper we ascend from the level of individual sentences on the top to the level of paragraphs on the bottom.     
In this study we attempt to systematically extract semantic features from paragraphs of text using a graphbased learning, assuming that adequate parsing trees for individual sentences are available. In our earlier studies Galitsky et al  Galitsky et al  we applied graph learning to parse trees at the sentence level, and here we proceed to learning the structure of paragraphs, relying on parse forest. We have defined the least general generalization of parse trees we call it syntactic generalization, and in this study we extend it to the level of paragraphs. We have applied generalizations of parse trees to the cases where a query is based on a single sentence, and candidate answers consist from single sentences  Galitsky et al  and multiple sentences Galitsky et al . In these cases, to rerank answers, we needed pairwise sentencesentence generalization and sentenceparagraph generalizations respectively. In this study we rely on PT to perform a paragraphlevel generalization, where both questions and answers are paragraphs of text.
   The contribution of this paper is to ascend from sentencelevel generalization to paragraphlevel generalization Fig.. On the top, the common part of two parse trees is shown, which is a generalization of two sentences. On the bottom, we show a number of mapped subgraphs which form the common parts of two PTs graphs, which is a generalization of two paragraphs. Generalizing paragraphs of text, we do not only have to match words and their syntactic links, but their discourse relations as well. Hence overall generalization structure is much more difficult than in the case of individual sentences. 
   The chart for how generalization of parse thickets supports search relevance is shown in Fig.  The PT generalization system inputs a search query and candidate answers obtained from local search index or using Bing search engine API. PT is computed for the query, which is a paragraph of text, and each candidate answer. Then each answer is generalized with the query to obtain the similarity score, which is the basis for relevance assessment. As a result, the answers with high similarity score is outputted as relevant. 
    We define the operation of generalization of text paragraphs via generalization of respective PTs to assess similarity between them. The use of generalization for similarity assessment is inspired by structured approaches to machine learning versus unstructured statistical alternatives where similarity is measured by a distance in feature space. Our intention is to extend the operation of least general generalization e.g., the antiunification of logical formulas towards structural representations of paragraph of texts. Hence we will define the operation of generalization on a pair of PT as finding the maximal common subthickets and outline two approaches to it
         To represent the structure of a paragraph of text, given parse trees of its sentences, we introduce the notion of Parse Thicket PT as a sequence of parse trees.
In this section we attempt to treat computationally, with a unified framework, two approaches to textual discourse
We selected these theories as most reliable and frequent sources of links between sentences. Although both these theories have psychological observation as foundations and are mostly of a noncomputational nature, we will build a specific computational framework for them. For RST, we will use explicit rules which will be applied to each sentence, attempt to extract an RST relation, and add a link to the PT. For SpActT, we use a vocabulary of communicative actions to find their subjects and add respective links to PT. In this section we provide the background for these theories of discourse and explain how to use them to build links between sentence building parse thickets. In Section we will show how to use these PT arcs for generalization operation.
People sometimes assume that whenever a text has some particular kind of discourse structure, there will be a signal indicating that structure. A typical case would be a conjunction such as ‘but’. What structure is seen depends vitally on the words and sentences of the text are, but the relationship between words and text structure is extremely complex. Phrases and syntactic patterns can also be used to signal discourse structure. 
  When one searches for document as an answer to a question, it can reside in multiple portions in this document. To link these portions automatically, one needs to link these portions somehow, and RST relation can be leveraged. Searching for a review on “white iPhone case”, it can read 
Here conjunction But indicates the connection between the First sentence and Fourth sentence, which happen to contain the phrase which matches white iPhone case. Notice that not all sentences containing keywords we seek can be matched if there is no corresponding discourse relation, white can refer to another object, not iPhone case.
   RST was originally developed as part of studies of computerbased text generation at Information Sciences Institute in by Bill Mann, Sandy Thompson and Christian Matthiessen. The theory is designed to explain the coherence of texts, seen as a kind of function, linking parts of a text to each other, which is employed in building PTs. This coherence is explained by assigning a structure to the text, which slightly resembles a conventional sentence structure. We adjust this structure for the purpose of multisentence search ability.
  Text parsing using RST has also been attempted, although not as enthusiastically. Marcu  presented an algorithm to parse the discourse structure of texts, using discourse markers as indicators of relations. Corston and Oliver  included other sources of information whether the span in question is a main, coordinate, or subordinate clause; position of clause main subordinate or subordinatemain; presence of certain adverbs; presence of pronouns; polarity of the clause, etc. Le and Abeysinghe  combine discourse markers, syntactic relations, and cohesive devices. Schilder  uses discourse markers and position, to parse discourse structure of a slightly different form, using Segmented Discourse Representation Theory Asher and Lascarides . 
We now introduce the relations of RST. Discourse units are clauses with verbs and its obligatory arguments, sentences, titles, and section titles even when they are verbless. Both restrictive and nonrestrictive modifiers that use a finite verb are embedded units. Adjacent text spans are linked together via rhetorical coherence relations. Mononuclear relations hold between text spans, where one of them, the nucleus, is more salient to the discourse structure, and the other one, the satellite, represents supporting information. If you show them this movie  they’ll scream.
Multinuclear hold between or more text spans, each of which has the same weight in the discourse structure. [Go to url. Find menu item register. Type your name]
Mononuclear relation links nucleus with its satellite. As an answer, satellite is only valid as long as satellite is present. 
We now introduce the rules to extract RST relations between elementary discourse units in the following form
 is a set of  phrases which will be linked. Partof Syntactic template is a set of sublists of Syntactic template phrases.
For the purpose of building PT, we construct syntactic templates to express RST Relational classes. We don’t have to cover all RST relation, and we don’t have to be precise in establishing them, unless relation type is matched with query term. We enumerate the relations and give example of some templates we use to detect an RST links, and specify respective linking rules.
Once rhetoric relation in one text is found, it can be generalized with this relation in another text. We propose a definition to do that, which will be the part of overall PT generalization
iRST relations in text,  jRST relations in text. Notice that individual RST generalization will be defined in Section .
In this section we formulate the Theory of Speech Act in a way applicable to finding links between sentences for paragraphlevel generalization. In this theory, dialogue, negotiation, conflict dispute are forms of interactions between human agents. Elements of the language which expresses these interactions are referred to as locutions, speech acts Bach & Harnish , utterances, or communicative actions we are going to keep using the last term and use abbreviation CA.
    How can CAs serve as links between sentences and help answer questions? For the same question we used in the previous Section, white iPhone case, let us consider a document “I asked them about cases for iPhone … I needed it to protect my phone … Sentence … Sentence  They answered that they had a white case”. What is important here is the link between CA asks and CA answered, passing through multiple sentences. The fact that the latter CA is a reaction response to the former is a basis for our conclusion that the subjects of these CAs { cases for iPhone, had a white case}  are correlated. Hence the phrases expressing these subjects can be linked, and matched together against a question. To determine that CAis a reaction to CA we need to establish their attributes, leveraging Speech Act Theory.
where verb characterizes some kind of interaction between people e.g., explain, confirm, remind, disagree, deny, etc., agent identifies the person , and subject refers to the information transmitted or object described, and cause refers to the motivation or explanation for the subject.
  The foundation of the current theory of Speech Acts was developed by Austin  where  he explores the performative utterances, aiming to prove that when people speak, they are doing more than simply conveying information—they act. A speech act is essentially a theory that asserts the claim that in saying something, we perform something. It is an action that is performed by means of language. An example would be a performative act of a judge during a hearing when he says “I now pronounce that the case is solved.”  Due to Austin’s designation of speech acts, sentences like this adopt a notion of action. The judge’s sentence is not a report of the action; it is the action indeed. Austin distinguishes between three types of linguistic acts the act of saying something, what one does in saying it, and what one does by saying it. He labels them Locutionary,  Illocutionary, and Perlocutionary, respectively Farrell . A locutionary act is simply saying something about the world, e.g. a declarative sentence such as “The product does not work.” This sentence is not posing a question, promising, or commanding anything. It simply states something about the world, containing purely propositional content. This type of act is the most basic, and does not require much more explanation.
        The illocutionary act includes promising, questioning, admitting, hypothesizing, etc. The locutionary act was simply the act of saying something, while the illocutionary act is performed in saying something. For example, “A company promises to support the product after it is sold” asserts more than simply stating a sentence about the world. It includes an assertion that is performative in nature. Illocutionary acts are very prominent in language, and are frequently in use in complaint scenarios.  The third type of linguistic acts are perlocutionary ones. These are nonconventional sentences that cause a natural condition or state in a person. These acts deemphasize the actual intentions, and focus on the effects on the hearer. For example, acts of frightening or convincing depend on the response of another person. If a perlocutionary act is successful, then one can state that an illocutionary acts has successfully taken place.
         Approximating scenarios of multiagent interactions, we follow along the lines of communicative actions’ division into constatives and performatives.  
· Constatives describe or report some state of affairs such that it is possible to assess whether they are false or true.
· Performatives, on the other hand, are fortunate or unfortunate, sincere or insincere, realistic or unrealistic, and, finally valid or invalid, which is the focus of the current study. Performatives address the attitude of an the agent performing the linguistic act, including his thoughts, feelings, and intentions.
To choose communicative actions to adequately represent an interaction between humans, we have selected the most frequently used ones from our structured database of complaints Table  Galitsky et al .  
Agree, explain, suggest, remind, allow, try, request, understand, inform, confirm, ask, check, ignore, convince, disagree, appeal, deny, threaten, bring to customer’s attention, accept complaint, accept /deny responsibilities, encourage, cheat
      A number of computational approaches have attempted to discover and categorize how the agents’ attitudes and communicative actions are related to each other in the case of computational simulation of human agents Searle  Cohen & Levesque . Applying machine learning to the attitudes and communicative actions, we are primarily concerned with how these approaches can provide a unified and robust framework for computing similarity between the communicative actions. The theory of speech acts seems to be one of the most promising approaches to categorizing communicative actions in terms of their roles. Following Bach & Harnish , we consider four categories of illocutionary communicative actions with major representatives stating, requesting, promising and apologizing. Each speech act is related to a single category only in the framework of the speech act theory. 
    To extend the speech act–based means of expressing similarity between communicative actions, we introduce five attributes each of which reflects a particular semantic parameter for communicative activity
· Positive/ negative attitude expresses whether a communicative action is a cooperative friendly, helpful move , uncooperative unfriendly, unhelpful move , neither or both hard to tell, .
· Request / respond mode specifies whether a communicative action is expected to be followed by a reaction , constitutes a response follows a previous request, neither or both hard to tell, .
· Info supply / no info supply tells if a communicative action brings in an additional data about the conflict , does not bring any information ,  does not occur here.
· High / low confidence specifies the confidence of the preceding mental state so that a particular communicative action is chosen, high knowledge/confidence , lack of knowledge/confidence , neither or both is possible .
· Intense / relaxed mode says about the potential emotional load high , low ,  neutral  emotional loads are possible.
    In order to define a robust framework to identify CA arcs in PT we are interested in distinguishing which are the most common communicative actions used in complaint scenarios, and how they can be clustered in terms of the attitudes commonly associated with them. Table shows the attribute of CA well suited for generalization. For example, agree^accept = <    >
Having built the similarity model for communicative actions, we can now compute wordword similarity for them via attributes. 
To measure the similarity of abstract entities expressed by logic formulas, a leastgeneral generalization Plotkin  was proposed for a number of machine learning approaches, including explanationbased learning and inductive logic programming. Given two logical terms, it produces a more general term that covers both. 
Definition . Let Eand Ebe two terms. Term E is a generalization of Eand Eif there exist two substitutions and such that E = Eand E = E The most specific generalization of Eand Eis called the antiunifier. Here, we apply this abstraction to antiunify such data as texts that are traditionally referred to as unstructured. 
    In this study, to measure the similarity between portions of text such as paragraphs, sentences and phrases, we extend the notion of generalization from logic formulas to the sets of syntactic parse trees of these portions of text. If it were possible to define the similarities between natural language expressions at a purely semantic level, the leastgeneral generalization would be sufficient. However, in horizontal search domains where the construction of full ontologies for a complete translation from natural language to logic language is not plausible, an extension of the abstract operation of generalization to the syntactic level is required. Rather than extracting common keywords, the generalization operation produces a syntactic expression that can be semantically interpreted as a common meaning shared by two sentences.
 Let us represent a meaning of two NL expressions using logic formulas and then construct the unification and antiunification of these formulas. How can we express a commonality between the expressions?
To express the meanings, we use the predicates cameraname_of_feature, type_of_users in real life, we would have a much higher number of arguments, and zoomtype_of_zoom. The above NL expressions will be represented as follows
where the variables uninstantiated values that are not specified in NL expressions are capitalized. Given the above pair of formulas, unification computes their most general specialization camerazoomdigital, beginner, and antiunification computes their most specific generalization, camerazoomAnyZoom, AnyUser.
 We eliminate the expressions in square brackets because they occur in one expression and do not occur in another. Thus, we obtain
The purpose of an abstract generalization is to find the commonality between portions of text at various levels. The generalization operation occurs on the following levels
At each level except the lowest one, the result of the generalization of two expressions is a set of expressions. In such a set, expressions for which lessgeneral expressions exist are eliminated. The generalization of two sets of expressions is a set of the sets that are the results of the pairwise generalization of these expressions.
lemmaw ^ lemmaw is either  ww, if the words are the same, or ‘’ otherwise a placeholder for an arbitrary word, if words wand ware different.
posw ^ posw is either partofspeech posw ^ posw, or ‘’ otherwise if their partsofspeech are different.
· For noun phrases, only those with the same head noun can be generalized. The generalization result includes this head noun. The head of a phrase is the word that determines the syntactic type of that phrase or analogously the stem that determines the semantic category of a compound of which it is a part. The rest of the words in phrases is generalized according to Definition .
For other types of phrases, generalization occurs analogously. Notice that English is primarily a headinitial language. Structure is descending as speech and processing move from left to right. Most dependencies have the head preceding its dependents, although there are also headfinal dependencies in parse trees. For instance, the determinernoun and adjectivenoun dependencies are headfinal as well as the subjectverb dependencies. Most other dependencies in English are, however, headinitial; the mixed nature of headinitial and headfinal structures is common across languages Lehmann .
    We outline the algorithm for generalization two sentences below, which concerns paths of syntactic trees rather than subtrees because these paths are tightly connected with language phrases. Regarding the operations on trees, we follow the work of Kapoor & Ramesh .   
       Although it is a formal operation on abstract trees, the generalization operation yields semantic information about the commonalities between sentences. Rather than extracting common keywords, the generalization operation produces a syntactic expression that can be semantically interpreted as a common meaning shared by two sentences.
 Obtain the parsing tree for each sentence. For each word tree node, we have a lemma, a part of speech and the form of the word’s information. This information is contained in the node label. We also have an arc to the other node.  
 Split sentences into subtrees that are phrases for each type verb, noun, prepositional and others. These subtrees are overlapping. The subtrees are coded so that the information about their occurrence in the full tree is retained.
 For each pair of subtrees, yield an alignment Gildea , and generalize each node for this alignment. Calculate the score for the obtained set of trees generalization results. 
 For each pair of subtrees of phrases, select the set of generalizations with the highest score the least general.
 Form the sets of generalizations for each phrase type whose elements are the sets of generalizations for that type.
 Filter the list of generalization results for the list of generalizations for each phrase type, exclude more general elements from the lists of generalization for a given pair of phrases.
 For a given pair of words, only a single generalization exists; if words are the same in the same form, the result is a node with this word in this form. We refer to the generalization of words occurring in a syntactic tree as a word node. If the word forms are different e.g., one is single and the other is plural, only the lemma of the word remains. If the words are different and only the parts of speech are the same, the resultant node contains only the partofspeech information with no lemma. If the parts of speech are different, the generalization node is empty. 
  For a pair of phrases, the generalization includes all the maximum ordered sets of generalization nodes for the words in the phrases so that the order of words is retained. Consider the following example
The generalization is {<JJdigital, NNcamera>, <NNtoday, ADV,Monday>}, where the generalization for the noun phrase is followed by the generalization for the adverbial phrase. The verb buy is excluded from both generalizations because it occurs in a different order in the above phrases. Buy  digital  camera is not a generalization phrase because buy occurs in a different sequence in the other generalization nodes.
We can see that the multiple maximum generalizations occur depending on how the correspondence between words is established; multiple generalizations are possible. To obey the condition of the maximum, we introduce a score for generalization. The scoring weights of generalizations are decreasing, roughly, in the following order nouns and verbs, other parts of speech, and nodes with no lemma, only a part of speech. In its style, the generalization operation follows the notion of the ‘leastgeneral generalization’ or antiunification, if a node is a formula in a language of logic. Therefore, we can refer to the syntactic tree generalization as an operation of antiunification of syntactic trees.
To optimize the calculation of the generalization score, we conducted a computational study to determine the POS weights and deliver the most accurate similarity measure possible between sentences Galitsky et al  Galitsky et al . The problem was formulated as finding the optimal weights for nouns, adjectives, verbs and their forms such as gerund and past tense so that the resultant search relevance is maximized. The search relevance was measured as a deviation in the order of search results from the best result for a given query delivered by Google. The current search order was determined based on the score of generalization for the given set of POS weights the other generalization parameters having been fixed. As a result of this optimization, we obtained WNN = , WJJ = , WRB = , WCD = , WVB = , and WPRP = , excluding common and frequent verbs like get/ take/set/put, for which WVBcommon= . We also establish that W<POS,> = different words but the same POS, and W<,word> = the same word occurring as different POSs in two sentences.
Definition .The generalization score or the similarity between the sentences sentand sent can then be expressed as a sum through the phrases of the weighted sum for  the words wordsentand word sent
The maximal generalization can then be defined as the generalization with the highest score. Accordingly, we define the generalization for phrases, sentences and paragraphs. The result of the generalization can be further generalized with other parse trees or generalizations. For a set of sentences, the totality of the generalizations forms a lattice the order of the generalizations is set by the subsumption relation and the generalization score. We enforce the associativity of the generalization of parse trees by means of computation it must be verified and the resultant list must be extended each time a new sentence is added. Note that such an associativity is not implied in our definition of generalization.  
We have manually created and collected a rule base for generic linguistic phenomena from various resources. Unlike a text entailment system, in our situation we do not require a complete transformation system as long as we have a sufficiently rich set of examples. Syntacticbased rules capture the entailment inferences associated with common syntactic structures, including the simplification of the original parse tree, reducing it into a canonical form, extracting its embedded propositions, and inferring propositions from the nonpropositional subtrees of the source tree Table .
Digital zoom, a feature provided by the new generation of cameras, dramatically decreases the images’ sharpness 
A cell phone can be easily grasped in the palm of a hand The hand can easily grasp the cell phone in its palm
Sony’s LCD screens work as well as Canon’s in a sunny environment The LCD of Sony… as well as that of Canon
Table  Rules of graph reduction for generic linguistic structures. The resultant reductions are italicized. 
The valid matching of sentence parts embedded as verb complements depends on the verbs’ properties and the polarity of the context in which the verb appears positive, negative, or unknown. We used the list of verbs for communicative actions in Galitsky and Kuznetsov , which indicate positive polarity context. This list is complemented with a few reporting verbs, such as say and announce, because opinions are often provided in reported speech in the news domain, while authors are usually considered reliable. We also used annotation rules to mark the negation and modality of predicates mainly verbs based on their descendent modifiers. 
An important class of transformation rules involves noun phrases. The adjectives of a single noun group can be resorted, as well as all nouns except the head one. A noun phrase that is a postmodifier of the head noun of a given phrase can be merged with the latter. The resultant meaning may be distorted; otherwise, we would miss important commonalities between expressions containing noun phrases. For an expression ‘NP<of or for> NP, we form a single NP with the head noun headNP,  headNP playing the role of modifier, and an arbitrary sorting of adjectives. For example, we convert ‘camera with digital zoom’ into ‘digital zoom camera’, headNP = camera. 
To rank search results, one needs to have a measure of similarity between questions and answers. Once a candidate set of answers is obtained by a keyword search using, for example TFIDF model, we calculate the similarity between the question and each of its candidate answers, and rank the latter set respectively, in the order of similarity decrease.
· Baseline bagofwords approach, which computes the set of common keywords/ngrams and their frequencies. 
· Pairwise sentence matching we will apply syntactic generalization to each pair of sentences, and sum up the resultant commonalities. This technique has been developed in our previous work Galitsky et al  Section .
The first approach is most typical for industrial NLP applications today, and the second one has been explored in our previous studies. Kernelbased approach to parse tree similarities Moschitti  Zhang et al , as well as tree sequence kernel Sun et al , being tuned to parse trees of individual sentences, also belong to the second approach. We demonstrate its richness of the third approach here, and in the consecutive sections we will provide a stepbystep illustration for how PTs are constructed and generalized. We will introduce a pair of short texts articles and compare the above three approaches. This example will go through the whole section.
The first paragraph can be viewed as a search query, and the second paragraph can be viewed as a candidate answer. A relevant answer should be a closely related text on one hand, and not a piece of duplicate information on the other hand.
Operator ‘’ in the following example and through all the paper denotes generalization operation. Describing parse trees we use standard notation for constituency trees […] represents subphrase, NN, JJ, NP etc. denote partsofspeech and types of subphrases, ‘’ will be is a placeholder for a word, part of speech, or an arbitrary graph node.
"UN nuclear watchdog passes a resolution condemning Iran for developing a second uranium enrichment site in secret",
"A recent IAEA report presented diagrams that suggested Iran was secretly working on nuclear weapons",
"Iran envoy says its nuclear development is for peaceful purpose, and the material evidence against it has been fabricated by the US",
"UN passes a resolution condemning the work of Iran on nuclear weapons, in spite of Iran claims that its nuclear research is for peaceful purpose",
"Envoy of Iran to IAEA proceeds with the dispute over its nuclear program and develops an enrichment site in secret",
"Iran confirms that the evidence of its nuclear weapons program is fabricated by the US and proceeds with the second uranium enrichment site"
The list of common keywords gives a hint that both documents are on nuclear program of Iran, however it is hard to get more specific details
Iran, UN, proposal, dispute, nuclear, weapons, passes, resolution, developing, enrichment, site, secret, condemning, second, uranium 
   [NNwork IN INon JJnuclear NNSweapons ],   [DTthe NNdispute INover JJnuclear NNS ],  [VBZpasses DTa NNresolution ],  
Parse Thicket generalization gives the detailed similarity picture which looks more complete than the pairwise sentence generalization result above
[NNgeneralization <Iran/envoy to UN>  Communicative_action  NNIran NNnuclear NN VBZis INfor JJpeaceful NNpurpose ],   
[NNgeneralization <Iran/envoy to UN>  Communicative_action  NNevidence INagainst NN Iran NNnuclear   VBNfabricated INby DTthe NNPus ]
One can feel that PTbased generalization closely approaches human performance in terms of finding similarities between texts. To obtain these results, we need to be capable of maintaining coreferences, apply the relationships between entities to our analysis subject vs relationtothis subject, including relationships between verbs develop is a partial case of work. We also need to be able to identify communicative actions and generalize them together with their subjects according to the specific patterns of Speech Act Theory. Moreover, we need to maintain Rhetoric Structure relationship between sentences, to generalize at the level of textual discourse.
Fig. and Fig. show the dependencybased parse trees for the above texts Tand T Each tree node has labels as partofspeech and its form such as SG for ‘single’; also, tree edges are labeled with the syntactic connection type such as ‘composite’. We are not concerned with a particular type of a parse tree representation dependency or constituency, as long as we have labels for nodes and vertexes.
We now proceed to an example of search query and two answers, where coreference establishes a link between two sentences and makes the linked words relevant to a query. We will consider two cases for text indexing, where establishing proper coreferences inside and between sentences connects entities in an index for proper match with a question
Text for indexing/candidate answer  … Tuberculosis is usually a lung disease. It is cured by doctors specializing in pulmonology. 
Text for indexing/candidate answer  … Tuberculosis is a lung disease… Pulmonology specialist Jones was awarded a prize for curing a special form of disease. 
In the first case, establishing the coreference link Tuberculosis → disease → is cured by doctors pulmonologists helps to match these entities with the ones from the question. In the second case this portion of text does not serve as a relevant answer to the question, although it includes keywords from this question. Hence at indexing time, keywords should be chained not just by their occurrence in individual sentences, but additionally on the basis of coreferences. One can observer that building PT for an answer online and matching it with question delivers answers which would be determined as irrelevant without building coreference chains. This way coreferences and same entity relations as parts of PTs improve search recall. From now on we will refer to such chains as phrases from distinct sentences as thicket phrases. 
We introduce a domain where a pairwise comparison of sentences is insufficient to properly learn certain semantic features of texts. This is due to the variability of ways information can be communicated in multiple sentences, and variations in possible discourse structures of text which needs to be takes into account.
   We consider an example of text classification problem, where short portions of text belong to two classes
I rent an office space. This office is for my business. I can deduct office rental expense from my business profit to calculate net income.
To run my business, I have to rent an office. The net business profit is calculated as follows. Rental expense needs to be subtracted from revenue.
    To store goods for my retail business I rent some space. When I calculate the net income, I take revenue and subtract business expenses such as office rent.
    I rent out a first floor unit of my house to a travel business. I need to add the rental income to my profit. However, when I repair my house, I can deduct the repair expense from my rental income.
   I receive rental income from my office. I have to claim it as a profit in my tax forms. I need to add my rental income to my profits, but subtract rental expenses such as repair from it.
   I advertised my property as a business rental. Advertisement and repair expenses can be subtracted from the rental income. Remaining  rental income needs to be added to my profit needs to be reported as taxable profit. 
Firstly, note that keywordbased analysis does not help to separate the first three paragraph and the second three paragraphs. They all share the same keywords rental/office/income/profit/add/subtract. Phrasebased analysis does not help, since both sets of paragraphs share similar phrases
   Anaphora resolution is helpful but insufficient. All these sentences include ‘I’ and its mention, but other links between words or phrases in different sentences needs to be used. 
    Rhetoric structures need to come into play to provide additional links between sentences. The structure to distinguish between 
renting to someone and adding to income embraces multiple sentences. The second clause about adding/subtracting incomes is linked by means of  the rhetoric relation of elaboration with the first clause for landlord/tenant. This rhetoric relation may link discourse units within a sentence, between consecutive sentences and even between first and third sentence in a paragraph. Other rhetoric relations can play similar role for forming essential links for text classification.
Once we have a sequence of parse trees for a question, and that of an answer, how can we match these sequences? A number of studies compute pairwise similarity between parse trees Collins and Duffy,  Punyakanok et al.,  Moschitti, . However, to rely upon discourse structure of paragraphs, and to avoid dependence on how content is distributed through sentences, we represent the whole paragraphs of questions and answers as a single graph. To determine how good is an answer for a question, we match their respective PTs and assign a score to the size of common subPT.
We extend the syntactic relations between the nodes of the syntactic dependency parse trees towards more general text discourse relations. Once we have such relations as “the same entity”, “subentity”, “superentity” and anaphora, we can extend the notion of phrase to be matched between texts. In case of single sentences, we match noun, verb, and other types of phrases in questions and answers. In case of multiple sentences in each, we extend the notion of phrases so that they are independent of how information being communicated is split into sentences. Relations between the nodes of parse trees which are other than syntactic can merge phrases from different sentences or from a single sentence, which are not syntactically connected. We will refer to such extended phrases as thicket phrases.
If words X and Y are connected by a coreference relation, an index needs to include the chain of words X XX, YY Y, where chains X XX and YY Y are already indexed phrases including X and Y. Hence establishing coreferences is important to extend index in a way to improve search recall. Usually, keywords from different sentences can only be matched with query keywords with a low score high score is delivered by intersentence match.
If we have two parse trees  and  of text , and an arc for a relation rP→ P  between the nodes P and P, we can now match of  against a phrase of a single sentence or a merged phrases of multiple sentences from .
  An example of building a thicket phrase by linking two trees is shown in Fig.  A thicket phrase can be thought as beginning from a phrase in one sentence, then jumping to the tree for another sentence and continuing the phrase.
Although the generalization is defined as the set of maximal common subgraphs, its computation is based on matching phrases. To generalize a pair of sentences, we perform chunking and extract all noun, verb, prepositional and other types of phrases from each sentence. Then we perform generalization for each type of phrases, attempting to find a maximal common subphrase for each pair of phrases of the same type. The resultant phraselevel generalization can then be interpreted as a set of paths in resultant common subtrees Galitsky et al., .
Generalization of parse thickets, being a maximal common subparse thicket, can be computed at the level of phrases as well, as a structure containing maximal common subphrases. However, the notion of phrases is extended now thicket phrases can contain regular phrases from different sentences. The way these phrases are extracted and formed depends on the source of nonsyntactic link between words in different sentences thicket phrases are formed in a different way for communicative actions and RST relations. Notice that the set of regular phrases for a parse thicket is a subset of the set of thicket phrases all phrases extracted for generalization. Because of this richer set of phrases for generalization, the parse thicket generalization is richer than the pairwise thicket generalization, and can better tackle variety in phrasings and writing styles, as well as distribution of information through sentences.
We will now outline the algorithm of forming thicket phrases. Most categories of thicket arcs will be illustrated below. 
Fig.  An arc which connects two parse trees for two sentences in a text on the top and the derived set of extended trees on the bottom. 
Figs. and show examples of Parse Thickets for the above paragraphs. In addition to syntactic links between words in a sentence, words in a paragraph are connected with links of the different nature. To form a complete formal representation of a paragraph, we attempt to express as many links as possible. The most obvious links are the same entities, which is a partial case of coreferences. A less trivial tasks is to identify a subentity relation; an external resource needs to be consulted. In Fig. we have a subentity link IAEA → UN. The fact that the former is a subentity in this case, suborganization is either obtained by using lookups available as a part of generalpurpose NLP system like OpenNLP, Stanford NLP Lee at al , GATE, LingPipe and others, or using web mining of sites like Wikipedia, Freebase and others. In more complex cases, such as multiwords, more complex web mining settings are required, such as Velásquez et al  Galitsky et al .
The communicative links reflect the discourse structure associated with participation or mentioning of more than single agent in text. The links form a sequence connecting the words for communicative actions either verbs or multiwords implicitly indicating a communicative intent of a person. We have thoroughly investigated the structure of how communicative actions occur in text in our study of argumentation in customer complaints, which is one of the most complicated cases of communicative actionsbased discourse Galitsky et al . 
   The main observation concerning communicative actions in relation to finding text similarity is that their subjects need to be generalized in the context of these actions, and that they should not be generalized with other, “physical” actions. Hence we generalize the individual occurrences of communicative actions together with their subjects, as well as their pairs as discourse “steps”. Generalization of communicative actions can also be thought from the standpoint if matching the verb frames, such as VerbNet Palmer . For a communicative action, we distinguish an actor, one or more agents being acted upon, and the phrase describing the features of this action. In the next section we will illustrate how respective arguments of verbs are generalized.
    Once we collected the thicket phrases for texts Tand T we can do the generalization. When we generalize thicket phrases from various categories, the following constraints should be taken into account Table . 
For example, entitybased thicket phrase can be generalized with regular phrases, but with neither RST nor SpActT.
In Section we defined and showed how to construct PTs. We also introduced generalization of individual parse trees. Based on that, in this section we introduce generalization of PTs which is based on generalization of individual parse trees on one hand and on generalization of discourse structures on the other hand. For coreferences and entityentity type of relation, we link them and consider the nodes in PT identical.  For RST, we attempt to extract an RST relation, and form a thicket phrase around it, including a placeholder for RST relation itself Galitsky et al . For SpActT, we use a vocabulary of communicative actions to find their subjects Galitsky & Kuznetsov , add respective arcs to PT, and form the respective sequence of thicket phrases.
Two connected clouds on the right of Fig.show the generalization instance based on RST relation “RCTevidence”. This relation occurs between the phrases 
evidenceforwhat  [Iran’s nuclear weapon program] and whathappenswithevidence [Fabricated by USA] on the rightbottom, and 
evidenceforwhat [against Iran’s nuclear development] and whathappenswithevidence [Fabricated by the USA] on the righttop.
Notice that in the latter case we need to merge perform anaphora substitution the phrase ‘ its nuclear development’  with ‘evidence against it’ to obtain ‘evidence against its nuclear development’.  Notice the arc it  development, according to which this anaphora substitution occurred. Evidence is removed from the phrase because it is the indicator of RST relation, and we form the subject of this relation to match. Furthermore, we need another anaphora substitution  its Iran to obtain the final phrase.
Green clouds at indicate the subPTs of  Tand Twhich are matched. We show three instances of PT generalization.
when typetype otherwise . Notice that the relation itself is retained to be further generalized if required.
Communicative actions are used by text authors to indicate a structure of a dialogue or a conflict Searle . Hence analyzing the communicative actions’ arcs of PT, one can find implicit similarities between texts. We can generalize
 one communicative actions from with its subject from Tagainst another communicative action with its subject from Tcommunicative action arc is not used ;
 a pair of communicative actions with their subjects from Tagainst another pair of communicative actions from Tcommunicative action arcs are used .
condemn  [‘Iran for developing second enrichment site in secret’] vs condemn [‘the work of Iran on nuclear weapon’] , or different communicative actions with similar subjects. 
Looking on the left bottom of Fig. one can observe two connected clouds the two distinct communicative actions dispute and condemn have rather similar subjects ‘work on nuclear weapon’. Generalizing two communicative actions with their subjects follows the rule generalize communicative actions themselves, and ‘attach’ the result to generalization of their subjects as regular subtree generalization. Two communicative actions can always be generalized, which is not the case for their subjects if their generalization result is empty, the generalization result of communicative actions with these subjects is empty too. The generalization result here for the case above is
Generalizing two different communicative actions Fig.  is based on their attributes and is presented elsewhere Galitsky et al .
suggest [Iran is secretly working on nuclear weapon] ↔  proceed [develop an enrichment site in secret] 
gives zero result because the arguments of condemn from Tand Tare not very similar. Hence we generalize the subjects of communicative actions first before we generalize communicative actions themselves.
We conclude the section by definition of generalization for a pair of CAs, and also for a pair of pairs of CAs.
We have shown how to compute similarity between PTs via phrases. That was an approximation of finding similarity between two graphs, considering their selected paths which correspond to phrases. Now we focus on information lossfree approach, where we compute similarity between two graphs in a classical, linguistic domainindependent way. Similarity between graphs is measured by the size of the maximal common subgraph Koch .
    To find maximal subPT we use a reduction of the maximal common subgraph problem to the  maximal clique problem Moon and  Moser  Bron and Kerbosch,  by constructing the edge product. The main difference with the traditional edge product is that instead of requiring the same label for two edges to be combined, we require nonempty generalization results for these edges. Though computationally not optimal, this approach is convenient for prototyping. Although for the trees this problem is On, for the general case of graphs finding maximum common subgraphs is NP hard Kann, . 
    We construct an edge product PT. Let  and  be PTs with nodes V and edges E, where  is a function assigning labels to the vertices, and L is a finite nonempty set of labels for the edges and vertices. The edge product PT  includes the vertex set  in which all edge pairs with and  have to produce nonempty generalization in their edge labels. Also, these edge pairs must produce nonempty generalizations in their corresponding end vertex labels. Let  and . The labels coincide if  and  and .
There is an edge between vertices  where  and  if two edge pairs are compatible, meaning that they are distinct and . Also, either condition for these edges of resultant PT should hold
 in  are connected via a vertex of the label which produces nonempty generalization with the label of the vertex shared by in . We label and call them cedges, or
To get a common subPT in  and  each edge pair in  and  vertex pair in  has to have generalizable label with all other edge pairs in  and  vertex pair in , which are forming a common subgraph. In this way a clique in  corresponds to common subPTs  and .
    A subset of vertices of G is said to be a clique if all its nodes are mutually adjacent. A maximal clique is one which is not contained in any larger clique, while a maximum clique is a clique having largest cardinality Vismara and Valery . After finding all maximal cliques for all pairs representing question and answer, for instance we look through all results and range them due to cliques cardinality. In this case the more edge pairs the result of generalization of two paragraphs contains the higher the relevance between two portions of text is.
   Applying some effective taking into account specific features of thicket graphs approaches to common subgraph problem is a subject of our future work. Also we are going to use more complex types of generalization such as pattern structures Ganter and Kuznetsov,  instead of pairwise generalization. The clique problem is the computational problem of finding a maximum clique, or all cliques, in a given graph, which is NPcomplete, one of Karps NPcomplete problems Karp . We build the edge product algorithm for PT case ouselves and integrated it with the available solution for the click problem of JGraphT library. 
    After finding all maximal common subgraph for all pairs representing question and answer for instance we rank results according to the score, based on size of common substructure and its linguistic properties. Each part of speech has its own score calculated from seacrch engine statistics.
Application of PT generalization for search occurs according to the following scenario. For the question and candidate answer, we build a pair of PTs. Then we perform generalization of PTs, either without loss of information, finding a maximum common PT subgraph, or with the loss of information, approximating the paths of resultant subgraph by generalizing thicket phrases. Search relevance score is computed accordingly as  a total number of vertexes in a common maximum subgraph in the first case, and calculating the number of words in maximal common subphrases, taking into account weight for parts of speech Galitsky et al , in the second case. This scenario will be evaluated in details in the section to follow.
    The system architecture is depicted in Fig.  There are three system components, which include Parse Thicket building, phraselevel processing, and graph processing.
   The textual input is subject to a conventional text processing flow such as sentence splitting, tokenizing, stemming, partofspeech assignment, building of parse trees and coreferences assignment for each sentence. This flow is implemented by either OpenNLP or Stanford NLP, and the parse thicket is built based on the algorithm presented in this paper. The coreferences and RST component strongly relies on Stanford NLP’s rulebased approach to finding correlated mentions, based on the multipass sieves.
   The graphbased approach to generalization relies on finding maximal cliques for an edge product of the graphs for PTs being generalized. As it was noticed earlier the main difference with the traditional edge product is that instead of requiring the same label for two edges to be combined, we require nonempty generalization results for these edges. Hence although the parse trees are relatively simple graphs, parse thicket graphs reach the limit of realtimes processing by graph algorithms.
   The system architecture serves as a basis of OpenNLP – similarity component, which is a separate Apache Software foundation project, accepting input from either OpenNLP or Stanford NLP. It converts parse thicket into JGraphT http//jgrapht.org/ objects which can be further processed by an extensive set of graph algorithms. In particular, finding maximal cliques is based on Bron and Kerbosch,  algorithm. Code and libraries described here are also available at http//svn.apache.org/repos/asf/opennlp/sandbox/opennlpsimilarity/. The system is ready to be plugged into Lucene library to improve search relevance. Also, a SOLR request handler is provided so that search engineers can switch to a PTbased multisentence search to quickly verify if relevance is improved.
   The second and third components are two different ways to assess similarity between two parse thickets. Phraselevel processing for the phrases of individual sentences has been described in detail in our previous studies Galitsky et al., . In this study we collect all phrases for all sentences of one paragraph of text, augment them with thicket phrases linguistic phrases which are merged based on the intersentence relation, and generalize against that of the other paragraph of text.
For two sets of thicket phrases, select a pair of phrases. Align them and find a maximal common subphrase
For two graphs, build a set of nodes for the edge product graph for all pairs of edges from PT graphs which can be generalized
PTs and their generalizations are important for domainindependent text relevance assessment. We propose a number of evaluation scenarios to compare the relevance performance of PTsupported search and its various forms of reduction. We demonstrate that simplifying construction of PT, discarding one or another linguistic feature, we lose relevance. We also compare the performance of complete PTsupported search with other stateoftheart question answering systems dealing with multisentence queries and /or multisentence questions.
In our earlier studies we explored the following cases for how generalization supports question answering
· Single sentence question against multiple sentences in the answer by pairwise generalization and summing up the score Galitsky et al .
We will reevaluate these cases in the unified framework to compare the results with the focus of the current paper, paragraphsized question against a paragraphsized answer or its snippet.
In this section we demonstrate that pairwise sentence generalization approach lacking discourse features can be outperformed by PT approach.
· Pairwise sentencesentence generalization ignoring intersentence links versus full PT generalization
In terms of complexity of queries and answers, we will proceed from single sentences queries/answers to multiple sentences queries/answers..
We conducted evaluation of relevance of syntactic generalization – enabled search engine, based on Bing search engine APIs. Instead of maintaining search index ourselves, for the purpose of evaluation we relied on Bing index and baseline search relevance. In terms of reproducibility of the experimental results in this study, since we measure a relative relevance improvement compared to Bing’s baseline, once we have a fixed publically available set of queries, it is acceptable. From Bing search results, we get page titles, snippets, and also extract paragraphs of text from the original webpage.
      For an individual query, the relevance was estimated as a percentage of correct hits among the first thirty, using the values {correct, marginally correct, incorrect} compare with Resnik, and Lin . Accuracy of a single search session is calculated as the percentage of correct search results plus half of the percentage of marginally correct search results. Accuracy of a particular search setting query type and search engine type is calculated, averaging through search sessions.
     For our evaluation, we use customers’ queries to eBay entertainment and productrelated domains, from simple questions referring to a particular product, a particular user need, as well as a multisentence forumstyle request to share a recommendation. In our evaluation we split the totality of queries into nounphrase class, verbphrase class, howto class, and also independently split in accordance to query length from keywords to multiple sentences. The evaluation was conducted by the authors.  To compare the relevance values between search settings, we used first search results obtained for a query by Bing API, and then reranked them according to the score of the given search setting syntactic generalization score.
   The list of products which serves the basis of queries is available at https//code.google.com/p/relevancebasedonparsetrees/downloads/detail?name=Querieset.xls. We took each product and found a posting somewhere on the web typically, a blog or forum posting about this product, requesting a particular information or addressing a particular user feature, need, or concern. From such extended expression containing product names, we formed the list queries of desired complexity.
   To estimate the statistical significance of results of relevance improvement, we estimate the standard deviation  of , the difference between the baseline average relevance and the one obtained by a particular reranking. For the evaluation set of search sessions for both baseline and AFPsupported search, we have
To do that, we assume that the search accuracy can be described by a normal distribution, and the number of searches is large enough Kohavi . 
There are a number of limitations in how the modern search engines attempt to find the occurrence of query keywords in a single sentence in a candidate search results. If it is not possible or has a low search engine score, multiple sentences within one document are used. However, modern search engines have no means to determine if the found occurrences of query keywords are related to each other, related to the same entity. Our first evaluation setting, the pairwise matching of parse tree for questions and answers, addressing this issue and showing that once parse trees are taken into account in addition to keywords, relevance is increasing. In our previous studies we already demonstrated this fact, and in this project we repeat this evaluation setting to conform to the consecutive ones based on PTs.
    Table shows the search relevance evaluation results for singlesentence answers.   The first and second columns show the types of phrases/sentences serving as queries. The third column shows the baseline Bing search relevancy. The fourth and fifth columns shows relevance of reranked search for snippets and original paragraphs from the webpage, and the sixth column shows relevance improvement compared with the baseline.
    One can observe that the higher the query complexity, the higher the impact of generalization for question and answer for reranking. For the simplest queries, there is no improvement. For  keywords phrases improvement starts being noticeable, of  and reaches  and  for two/three sentence queries respectively. As the absolute precision of search naturally drops when queries become more complex, relative contribution of syntactic generalization increases.
    In most cases, using original text is slightly better than using snippets, by about  except the case of the simple queries. In the case of  keywords the use of generalization is unreasonable, so the results can be even distorted and reranking by generalization score is not meaningful.
    We did not find a significant correlation between a query type, phrase type, and search performance with and without syntactic generalization for these types of phrases. Verb phrases in questions did well for multisentence queries perhaps because the role of verbs for such queries is more significant than for simpler queries where verbs can be frequently ignored.
   It is hard for modern search engines to determine for a given occurrence of keywords, being in different sentences, if they are related to each other, and are related to the query term. Once we establish links between words in different sentences, it should help in determining how these words are potentially connected with the query keywords. We will evaluate this observation in this section.
      Relevance improvement for the queries ranging from a few keywords to a whole sentence is shown in Table  One can see that the simplest cases of short query and a single compound sentence gives about  improvement. PTbased relevance improvement stays within  as query complexity increases by a few keywords, and then increases to  as query becomes a whole sentence. For the same query complexity, naturally, search accuracy decreases when more sentences are required for answering this query. However, contribution of PTs does not vary significantly with the number of sentences the answer occurs in one, two , or three.  PTsupported search outperforms the pairwise generalizationsupported search for the similar setting of  complex phrase to one sentence query Table rows  by  .
      We separately measure search relevance when PT is same/sub/superentity  based, coreferencesbased,  RSTbased and SpActTbased columns . Our hybrid approach includes all these sources for links altogether column . We consider all cases of questions phrase, one, two, and three sentences and all cases of search results occurrences compound sentence, two, and three sentences and measured how PT improved the search relevance, compared to original search results of Bing. 
       We found that contribution of intersentence links decreases in the following order coreferences, 
coreferences,  RST for the full sentence queries; the deviation between their contribution is within a percent or two. What is visible is that all of these sources of discourse information  in a standalone mode improve the relevance, hence their hybrid is indeed required for overall PTsupported search. This conclusion is made taking into account that these sources are independent relevance improvers, since they are based on mechanisms and linguistic theories of totally different natures.
We proceed to evaluation of how generalization of PTs can improve multisentence search, where one needs to compare a query as a paragraph of text against a candidate answer as a paragraph of text snippet. Evaluation results show the accuracies in percentages, averaging over searches. 
When the query complexity increases from one sentence to four sentences, the contribution of PT increases from  for a single sentence to  for double sentence and then to  for triple sentence and stays the same for four sentences in the query. Although as queries become more complicated, overall drop of PTsupported relevance is not lower than the respective drop of baseline relevance, the significance of the relevance improvement is obvious Table .
       We observe that contribution of intersentence links decreases in the following order SpActT , coreferences, same/sub/superentity,  and RST, and for two sentence queries, and SpActT, RST, coreferences, and same/sub/superentity. Hence for longer queries and answers, the role of discourse theories is higher than that of for simpler queries, where coreferences, and same/sub/superentity turned out to be more important.
  RST for the full sentence queries; the deviation between their contribution is within a percent or two.
Once we know that we should leverage all available discourserelated information for better search relevance, we investigate various approaches to generalization computation, and also explore performance in various domains where search is associated with multisentence questions and multisentence answers. Moreover, for each domain and query/answer complexity, once we obtain search results from Bing Search API, we either take its snippet or download the original webpage and extract the respective paragraph, to form its PT.
Product recommendation, where a user explicitly or implicitly according to a belief of an automated agent is requesting information about products. This agent finds a chat, and determines user intent from text analyzing epistemic states of this user, and mention that this user is seeking a productrelated recommendation. The input for such the recommendation is a few sentences of text. Given this input, a search request is sent to Bing Blogs and Forums and candidate recommendations are filtered out in terms of relevance by PT generalization. The data is collected from eBay.com and StubHub.com
Travel recommendation, where an agent reads chats about travel activities, things to do and hotels. The agent tracks travel chats, forums, blogs and trip reports and produces recommendation when requested or when user intent is detected.The data was collected at UpTake.com acquired by Groupon.com.
Facebook automated posting agent, which issues posts on  behalf of its human host. For the purpose of promoting social activity and enchance communications with the friends other than most close ones, this agent is authorized to comment on postings, images, videos, and other media. Given one or more sentence of user posting or image caption, this agent issues a Bing Web and Bing Blogs APIs search request and filters the results for relevance. Experiments with Facebook account were conducted using OpenGraph involving a number of personal accounts Fig. .
Fig.  The Facebook agent is posting a message on behalf of its human host. This message is obtained from Bing Blogs API and is supposed to be relevance unlike the most of Facebook ads on the right.
The value of PT based generalization varies from domain to domain significantly, mostly depending on the writing style and use of terminology by the authors presenting their opinions on the products. When things in a domain are named uniquely, and the typical writing style is plain enumeration of product features, contribution of PT is the least shopping product domains. On the contrary, where writing styles vary a lot and different people name the same things differently, in such horizontal domain as Facebook, the baseline relevance is low, the resultant relevance is lower  than in the other domains . 
One can see that contribution of SpAct source varies from domain to domain. In product and travel recommendations, its contribution is low about . At the same time, in the Facebook domain, where the description of interaction between people do occur, its contribution reaches .
Proceeding from Snippets to original paragraphs in a webpage gives further  increase for both thicket phrasebased and graphbased computation of PT.
Graphbased algorithms Bunke et al  Conte et al  for PT generalization are associated with much higher complexity than phrase based. At the same time, loss of information for a phrase base approach leads to less than  drop in the resultant relevance. Hence we select the phrasebased approach to PT generalization as most efficient.
Table  Evaluation results for various search domains and for various implementations of PT generalization
Relevance of  PT/phrase  generalization search, , averaging over searches, using original text, without SpAtcT
In this section we evaluate the search task which is not specific to the given study, but constitutes a standard testbed for question answering systems. We obtained a list of entitybased queries, some of which require an answer contained in multiple sentences. The evaluation set of questions was obtained from 
We took queries from this list and converted into short phrases, longer phrases, and extended by  sentences, to match the above evaluation cases. There are also search sessions per query/ answer types.
These search evaluation settings for Table were inspired by TREC  whose goal was to perform entityoriented search tasks on the web. Many user information needs concern entities people, organizations, locations, products, etc. which are better answered by returning specific objects instead of just any type of documents.  Like Trec Entity track, we used normalized Discounted Cumulative Gain NDCG, the normalized discounted cumulative gain at rank R the number of primary and relevant search results for that topic where primary pages get gain and relevant pages get gain  This is unlike our previous evaluation settings.
     The intuition behind the Discounted Cumulative Gain approach is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position  is defined as
Where relis the relevance of the first result, and  reli  is the relevance of the consecutive  results i< Dividing the obtained DCG by the DCG of the ideal ranking, we obtain a normalized DCG.
TREC evaluation was conducted in a similar environment, relying on Bing, Yahoo, and other web knowledge sources.
   In the example search result Fig.  PTsupported search reranks the snippets to find the entity occurring in an expression [ENTITY is  <query with substituted WHword>]. In most cases, ENTITY occurs in a different sentence to the one which is best matched with <query with substituted WHword>.
Fig.  Bing search results for the entitybased query with expected answer “Michelangelo’s Pieta“, obtained by APFsupported search
   We compare the results of PTbased search with that of the TREC Entity Track participants Table . The best teams, BIT Jiang et al  and FDWIM had slightly higher relevance, NDCG  and , respectively, and the rest of the teams, including Purdue, NiCT , ICTNET, and  UWaterlooEng obtained a lower relevance compared to APFbased approach Balog et al . In the current study for the hybrid RSTSpActT forest generalization approach we obtained NDCG  , ,  for sentence answers, sentence answers and sentence answers respectively. It worth mentioning that the above approaches are oriented at answering entitybased questions whereas the current approach targets the cases where found keywords are distributed through multiple sentenced in the search result snippet. This evaluation covers the overlap of these two cases, and we believe PT performance is satisfactory here.
   For the entity based search from the TREC Entity Track queries the improvement of search by using PT and especially RST is higher than for the search in Section  for both short and long queries. This is due to the fact that entitybased questions take advantage of the coreferences and RST relations such as elaboration, which are typical in the paragraphs of text answering entitybased questions. Since both short and long entitybased phrases heavily rely on the coreferences and RST, there is a relatively uniform improvement of search accuracy, compared to the search in previous evaluation subsections where PT contribution for more complicated questions is higher.
     Over the past few years, complex questions have been the focus of much attention in the automatic questionanswering community. Most current complex QA evaluations included the AQUAINT Relationship QA Pilot, the TREC Relationship QA Task, and the TREC entity task Jiang et al  whose results we compared to ours, and Document Understanding Conference DUC. These evaluations require systems to return unstructured lists of candidate paragraphlength answers in response to a complex question that are responsive, relevant, and coherent. For the DUC settings, Chali et al  reports  improvement from  to  of parse tree similarity based approach over keywordbased lexical for the Kmeans framework Cahel et al  which roughly corresponds to our improvement of single sentence generalizationbased search over the baseline Section . 
     Moschitti and Quarteroni  report the accuracy Fmeasure on the TRECQA dataset of  ±  for a bagofwords classifier and  ±  for the optimal combination of tree kernels. If one reranks deteriorates search engine search results by keyword occurrence only, and then compare with the APF performance, a similar performance would be observed. Singlesentence generalization relies on similar linguistic information about questions and answering as tree kernel, and APF extension becomes noticeable compared to commercial search engine results rather than bagofwords systems.
     General pattern structures consist of objects with descriptions called patterns that allow a semilattice operation on them Ganter & Kuznetsov . In our case, for paragraphs of text to serve such objects, they need to be represented by structures like parse thickets, which capture both syntactic level and discourselevel information about texts. Pattern structures arise naturally from ordered data, e.g., from labeled graphs ordered by graph morphisms.  In our case labeled graphs are parse thickets, and morphisms are the mappings between their maximal common subgraphs. Besides finding maximal common subgraph, PTs can be viewed from the standpoint of graph search problem. For example, Mandow   describes a new general algorithm for graph search problems with additive lexicographic goals. The use of lexicographic goals in the formulation of search problems provides greater control and expressive power over the properties of solution paths in the algorithm, called METALAN.
     One of the first systems for the generation of conceptual graph representation of text is described in Sowa and Way, . It uses a lexicon of canonical graphs that represent valid possible relations between concepts. These canonical graphs are then combined to build a conceptual graph representation of a sentence. Since then syntactic processing has dramatically improved, delivering reliable and efficient results.  PTs can be viewed as conceptual graphs which are derived automatically from Concept mining has found a number of applications as well Bichindaritz, & Akkineni .
     Hensman & Dunnion  describes a system for constructing conceptual graph representation of text by using a combination of existing linguistic resources VerbNet and WordNet. However, for practical applications these resources are rather limited, whereas syntactic level information such as syntactic parse trees is readily available. Moreover, building conceptual structure from individual sentences is not as reliable as building these structures from generalizations of two and more sentences. 
    In this study we attempt to approach conceptual graph level Sowa  Polovina & Heaton  using pure syntactic information such as syntactic parse trees and applying learning to it to increase reliability and consistency of resultant semantic representation. The purpose of such automated procedure is to tackle information extraction and knowledge integration problems usually requiring deep natural language understanding Galitsky  and cannot be solved at syntactic level.
    Whereas machine learning of syntactic parse trees for individual sentences is an established area of research, the contribution of this paper is a structural approach to learning of syntactic information at the level of paragraphs. A number of studies applied machine learning to syntactic parse trees Collins & Duffy , convolution kernels being the most popular approach Haussler . Parse tree kernels are also used for plagiarism detection 
        Harabagiu et al.  introduce a new paradigm for processing complex questions that relies on a combination of question decompositions based on a Markov chain, factoid QA techniques, and multidocument summarization. The Markov chain is implemented by following a random walk with a mixture model on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topicrelevant passages that manifest these relations. Decomposed questions are then submitted to a stateoftheart QA system in order to retrieve a set of passages that can later be merged into a comprehensive answer. The authors show that question decompositions using this method can significantly enhance the relevance and comprehensiveness of summarylength answers to complex questions. This approach does not rely on the association between concepts available for decomposed simpler questions from the commercial search engines. In the current study we achieve relevance based on better match of questions to answers, obtained by search engines which have learned the best matches for decomposed questions relying on user selections. Hence in our evaluation settings we skip decomposition and do not do summarization, achieving higher relevance by finding relevant documents among the candidate set which has been formed by search engine APIs. Moreover, to the best of our knowledge, no approach to answering complex questions is relying on linguistic discourse theories.
     The evaluations in Harabagiu et al.  have shown that the question decompositions lead to more relevant and complete answers. Moreover, the coverage of auto generated question decompositions, when compared with the questions generated from the answer summary, are better indicators of answer quality than the relevance score to the complex question. The question coverage for automatic methods is  of the coverage of questions produced by humans. Within the framework of the current study, question decomposition occurs via matching with various parse trees in the answers. If the baseline answers are reranked by humans, the coverage recall is about  higher than the automated system in the lower section of Table where sentence questions are matched with sentence answers, recall values are not shown in the table.
   Use of PT allows for domainindependent search applications, desired more than three decades ago. Xing & Li  proposes a domainindependent approach to constructing natural language interfaces for applications. Domain independence provides generality to natural language interfaces. A hierarchical structure for natural language processing and a representation language, the Intermediate Carrier Language, are introduced. A special phase of natural language processing called domain compiling is also proposed and described.
   Learning of PTs can be viewed as learning cases in the framework of casebased reasoning. Richter compares casebased reasoning with other methods searching for knowledge, considering it as a resource that can be traded. It has no value in itself; the value is measured by the usefulness of applying it in some process. Such a process has infoneeds that have to be satisfied. The concept to measure this is the economical term utility. In general, utility depends on the user and its context, i.e., it is subjective. The author introduces the levels of contexts from general to individual and illustrates that CaseBased Reasoning on the lower, i.e., more personal levels is quite useful, in particular in comparison with traditional informational retrieval methods.
 Yang and Soo  develop a technique to extract conceptual graphs from a patent claim using syntactic information such as POS and dependency tree, as well as semantic information such as the background ontology Vicient et al . Due to extensive technical domain terms and long sentences in patent claims, it is difficult to apply a NLP Parser directly to parse the plain texts in the patent claim. This paper combines techniques such as finite state machines, PartOfSpeech tags, conceptual graphs, domain ontology and dependency tree to convert a patent claim into a formally defined conceptual graph. The method of a finite state machine splits a lengthy patent claim sentence into a set of shortened subsentences so that the NLP Parser can parse them one by one effectively. The PartOfSpeech and dependency tree of a patent claim are used to build the conceptual graph based on the preestablished domain ontology. The result shows that  subsentences split from patent claims can be efficiently parsed by the NLP Parser. 
Whereas machine learning of syntactic parse trees for individual sentences is an established area of research Haussler, ; Collins and Duffy,    Moschitti, , the contribution of this paper is a structural approach to learning of syntactic information at the level of paragraphs.  Galitsky   observed how employing a richer set of linguistic information, such as syntactic relations between words, assists relevance tasks. To take advantage of semantic discourse information, we introduced parse thicket representation and proposed the way to compute similarity between texts based on generalization of parse thickets. In this work we build the framework for generalizing PTs as sets of phrases to rerank search results obtained via keyword search.
The operation of generalization to learn from parse trees for a pair of sentences turned out to be important for search reranking. Once we extended it to learning parse thickets for two paragraphs, we observed that the relevance is further increased compared to the baseline Bing search engine API, which relies on keyword statistics in the case of multisentence query. Parse thicket is intended to represent the syntactic structure of text as well as a number of semantic relations for the purpose of the real time reranking of the search results. Parse thickets include relations between words in different sentences, such that these relations are essential to perform a broad match of queries and answers. 
We considered the following sources of relations between words in sentences coreferences, taxonomic relations such as subentity, partial case, predicate for subject etc., rhetoric structure relation and speech acts. We demonstrated that search relevance can be improved if search results are subject to confirmation by parse thicket generalization, where answers occur in multiple sentences. We showed that each source contributes on its own to improve relevance, and altogether intersentence links are fairly important for finding relevant complex answers to complex questions.
Traditionally, machine learning of linguistic structures is limited to keyword forms and frequencies. At the same time, most theories of discourse are not computational, they model a particular set of relations between consecutive states. In this work we attempted to achieve the best in both worlds learn complete parse tree information augmented with an adjustment of discourse theory allowing computational treatment.
To the best of our knowledge this is one of the first works in learning a semantic discourse to solve a search relevance problem, using on a sequence of parse trees. Instead of using linguistic information of individual sentences, we can now compute text similarity at the level of paragraphs. 
We have contributed the PTbased functionality to OpenNLP so that search engineers can easily plug it in their search infrastructure. The algorithms for PT construction, PT generalization via phrases and via graphs are available at https//code.google.com/p/relevancebasedonparsetrees.
One approach to learning parse trees is based on tree kernels, where the authors propose a technique oriented specifically to parse trees, and reduce the space of all possible subtrees. Partial tree kernel Moschitti,  allows partial rule matching by ignoring some child nodes in the original production rule. Tree Sequence Kernel or TSK Sun et al.,  adopts the structure of a sequence of subtrees other than the single tree structure, strictly complying with the original production rules. Leveraging sequence kernel and tree kernel, TSK enriches sequence kernel with syntactic structure information and enriches tree kernel with disconnected subtree sequence structures.
The approach of phraselevel matching for parse trees and thicket phrase matching for parse thickets turns out to be much more efficient than graph based approaches, including graph kernels. Instead of considering the space of all possible subgraphs, we consider paths in trees and graphs, which correspond to linguistic phrases, or to phrases merged according to intersentence relations of the discourse. We do not consider parts of trees or thickets which correspond neither to a phrase of parse tree nor to a thicket phrase of thicket. This is due to the observation that such subtrees and subgraphs are incomplete, redundant or noisy features to rely on, conducting learning. 
To estimate the complexity of generalization of two parse thickets, let us consider an average case with five sentences in each paragraph and words in each sentence. Such thickets have on average phrases per sentence, intersentence arcs, which give us up to thicket phrases each. Hence for such parse thickets we have to generalize up to linguistic phrases and thicket phrases of the first thicket against the set of similar size for the second thicket. Taking into account a separate generalization of noun and verb phrases, this average case consists of   generalizations, followed by the subsumption checks. Each phrase generalization is based on up to string comparisons, taking an average size of phrase as words. Hence on average the parse thicket generalization includes operations. Since a string comparison takes a few microseconds, thicket generalization takes on average milliseconds without use of index. However, in an industrial search application where phrases are stored in an inverse index, the generalization operation can be completed in constant time, irrespectively of the size of index Lin, . In case of mapreduce implementation Lin&Dyer  of generalization operation, for example, using Cascading framework, the time complexity becomes constant with the size of candidate search results to be reranked Dean, .
The author is grateful to Sergey Kuznetsov, Dmitry Ilvovsky, Fedor Strok, Boris Kovalerchuk, Daniel Usikov for the fruitful discussion and help in preparation of this manuscript.
 Bichindaritz, Isabelle, Sarada Akkineni. Concept mining for indexing medical literature. Engineering Applications of Artificial Intelligence, Volume  Issue  June  pp . http//dx.doi.org//j.engappai..
 Bron, Coen; Kerbosch, Joep , Algorithm  finding all cliques of an undirected graph, Commun. ACM ACM  .
 Bunke, H. GraphBased Tools for Data Mining and Machine Learning. Machine learning and data mining in pattern recognition. Lecture Notes in Computer Science,  Volume , . 
 Bunke, H.,  P.  Foggia,  C. Guidobaldi, C. Sansone,  and  M. Vento. A comparison of algorithms for maximum common  subgraph on randomly connected graphs. Structural, Syntactic, and Statistical Pattern Recognition, pages , 
 Byun, H., SeongWhan Lee.  Applications of Support Vector Machines for Pattern Recognition A Survey. In Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines SVM , SeongWhan Lee and Alessandro Verri Eds.. SpringerVerlag, London, UK, UK, .
 Conte, D. P.  Foggia,  and  M. Vento. Challenging complexity of maximum common  subgraph detection algorithms  A performance analysis  of three algorithms on a wide database of graphs. Journal of Graph Algorithms  and  Applications, ,  
 Conte, D. P. Foggia, C. Sansone, and M. Vento. Thirty years of graph matching in pattern recognition. International Journal of Pattern Recognition and Artificial Intelligence, Vol.  No.  .
 Dan Gusfield.  Algorithms on Strings, Trees and Sequences. Cambridge University Press, Cambridge, UK.
 Daniel Jurafsky, James H. Martin. Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. 
 Dean, Jeff. Challenges in Building LargeScale Information Retrieval Systems. research.google.com/people/jeff/WSDMkeynote.pdf 
 Domingos P. and Poon, H. Unsupervised Semantic Parsing, In Proceedings of the Conference on Empirical Methods in Natural Language Processing,  Singapore ACL.
 Ehrlich H.C., Rarey M. Maximum common subgraph isomorphism algorithms and their applications in molecular science review. Wiley Interdisciplinary Reviews Computational Molecular Science,  vol. , pp. .
 Finn, V.K.  On the synthesis of cognitive procedures and the problem of induction. NTI Series  N pp. .
 Fukunaga, K. Introduction to statistical pattern recognition d ed., Academic Press Professional, Inc., San Diego, CA, 
 Furukawa, K.  From Deduction to Induction Logical Perspective. The Logic Programming Paradigm. In Apt, K.R., Marek V.W., Truszczynski, M., Warren, D.S., Eds. Springer.
 Galitsky, B. Natural Language Question Answering System Technique of Semantic Headers. Advanced Knowledge International, Australia .
 Galitsky, B.,  Josep Lluis de la Rosa, Gábor Dobrocsi. Inferring the semantic properties of sentences by mining syntactic parse trees. Data & Knowledge Engineering. Volume , November  .
 Galitsky, B., Daniel Usikov, Sergei O. Kuznetsov Parse Thicket Representations for Answering Multisentence questions. h International Conference on Conceptual Structures, ICCS .
 Galitsky, B., Kuznetsov S, Learning communicative actions of conflicting human agents. J. Exp. Theor. Artif. Intell.   .
 Galitsky, B., G. Dobrocsi, J.L. de la Rosa, Kuznetsov, S.O. From Generalization of Syntactic Parse Trees to Conceptual Graphs, in M. Croitoru, S. Ferré, D. Lukose Eds. Conceptual Structures From Information to Intelligence, h International Conference on Conceptual Structures, ICCS  Lecture Notes in Artificial Intelligence, vol.  pp. .
 Galitsky, B., Gabor Dobrocsi, Josep Lluís de la Rosa, Sergei O. Kuznetsov Using Generalization of Syntactic Parse Trees for Taxonomy Capture on the Web. h International Conference on Conceptual Structures,  ICCS   .
 Galitsky, B., Content Inversion for User Searches and Product Recommendation Systems and Methods. US Patent Application, eBay number  .
 Galitsky, B., Machine Learning of Syntactic Parse Trees for Search and Classification of Text. Engineering Applications of Artificial Intelligence,  http//dx.doi.org//j.engappai..
 Galitsky, B., MP González, CI Chesñevar. A novel approach for classifying customer complaints through graphs similarities in argumentative dialogue. Decision Support Systems,    .
 Ganter, B, Kuznetsov SO Pattern Structures and Their Projections. In Conceptual Structures Broadening the Base. Lecture Notes in Computer Science Volume   pp .
 Gildea. D.   Loosely treebased alignment for machine translation. In Proceedings of the h Annual Conference of the Association for Computational Linguistics ACL, pp. , Sapporo, Japan.
 Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu and Dan Jurafsky. Deterministic coreference resolution based on entitycentric, precisionranked rules. Computational Linguistics , 
 HennigThurau, Thorsten, André Marchand, and Paul Marx. , Can Automated Group Recommender Systems Help Consumers Make Better Choices? Journal of Marketing, , .
 Hensman, S. and Dunnion, J. Automatically building conceptual graphs using VerbNet and WordNet. International Symposium on information and Communication Technologies, Las Vegas, Nevada, June ,  ACM International Conference Proceeding Series, vol.  Trinity College Dublin, pp., 
 Iwashita, Motoi, Shinsuke Shimogawa, Ken Nishimatsu. Semantic analysis and classification method for customer enquiries in telecommunication services. Engineering Applications of Artificial Intelligence, Volume  Issue  December  Pages  http//dx.doi.org//j.engappai..
 Kalervo,  Jarvelin, Jaana Kekalainen Cumulated gainbased evaluation of IR techniques. ACM Transactions on Information Systems ,  
 Kapoor, S and H. Ramesh, “Algorithms for Enumerating All Spanning Trees of Undirected and Weighted Graphs,” SIAM J. Computing, vol.  pp. , 
 Kann, V. On the Approximability of the Maximum Common Subgraph Problem. In STACS , Alain Finkel and Matthias Jantzen Eds.. SpringerVerlag, London, UK, UK, , 
 Karp, Richard M. , "Reducibility among combinatorial problems", in Miller, R. E.; Thatcher, J. W., Complexity of Computer Computations, New York Plenum, pp. . 
 Kim, JungJae, Piotr Pezik and Dietrich RebholzSchuhmann. MedEvi Retrieving textual evidence of relations between biomedical concepts from Medline. Bioinformatics. Volume  Issue pp. . 
 Koch,  I.  Enumerating all connected maximal common  subgraphs in two graphs. Theoretical Computer Science, ,  
 Kohavi, Ron. A Study of CrossValidation and Bootstrap for Accuracy Estimation and Model Selection. International Joint Conference on Artificial Intelligence IJCAI 
 Lin, J. and Chris Dyer, DataIntensive Text Processing with MapReduce. Morgan & Claypool Publishers, 
 Lin, J. DataIntensive Text Processing with MapReduce. intool.github.io/MapReduceAlgorithms/MapReducebookfinal.pdf , 
 Le, H.T. and Abeysinghe, G.  ‘A Study to Improve the Efficiency of a Discourse Parsing System’, in A. Gelbukh ed. Proceedings of h International Conference on Intelligent Text Processing and Computational Linguistics, Vol.  pp. . BerlinSpringer.
 Mann, William C., Christian M. I. M. Matthiessen and Sandra A. Thompson . Rhetorical Structure Theory and Text Analysis. Discourse Description Diverse linguistic analyses of a fundraising text. ed. by W. C. Mann and S. A. Thompson. Amsterdam, John Benjamins .
 Manning, C. and Hinrich Schütze, Foundations of Statistical Natural Language Processing, MIT Press. Cambridge, MA May 
 Marcu, D.  ‘From Discourse Structures to Text Summaries’, in I. Mani and M.Maybury eds Proceedings of ACL Workshop on Intelligent Scalable Text Summarization, pp. , Madrid, Spain.
 Montaner, M.; Lopez, B.; de la Rosa, J. L. June . A Taxonomy of Recommender Agents on the Internet. Artificial Intelligence Review  .
 Moschitti,A. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings of the h European Conference on Machine Learning, Berlin, Germany, 
 Mandow, L., J.L. Pérez de la Cruz. A heuristic search algorithm with lexicographic goals. Engineering Applications of Artificial Intelligence, Volume  Issue  December  pp . http//dx.doi.org//SX.
 Palmer, Martha. "Semlink Linking PropBank, VerbNet and FrameNet." Proceedings of the Generative Lexicon Conference. Sept.  Pisa, Italy GenLex 
 Plotkin, G.D. A note on inductive generalization. In B. Meltzer and D. Michie, editors, Machine Intelligence, volume  pages . Elsevier NorthHolland, New York, 
 Punyakanok, V., Roth, D., & Yih, W. . Mapping dependencies trees an application to question answering. In Proceedings of AI & Math, Florida, USA.
 Punyakanok, V.,Roth, D. and Yih, W. The Necessity of Syntactic Parsing for Semantic Role Labeling. IJCAI
 Robinson J.A. A machineoriented logic based on the resolution principle. Journal of the Association for Computing Machinery,  
 Richter, Michael M. The search for knowledge, contexts, and CaseBased Reasoning. Engineering Applications of Artificial Intelligence, Volume  Issue  February  Pages . http//dx.doi.org//j.engappai..
 Kuznetsov, SO and M.V. Samokhin, Learning Closed Sets of Labeled Graphs for Chemical Applications. In Proc. h Conference on Inductive Logic Programming ILP , Lecture Notes in Artificial Intelligence Springer, Vol. pp.., 
 Schilder, F.  ‘Robust Discourse Parsing via Discourse Markers, Topicality and Position’, Natural Language Engineering / .
 Searle, John.  Speech acts An essay in the philosophy of language. Cambridge, England Cambridge University.
 Son, JeongWoo, TaeGil Noh, HyunJe Song, SeongBae Park. An application for plagiarized source code detection based on a parse tree kernel. Engineering Applications of Artificial Intelligence, Volume  Issue  September  pp. . http//dx.doi.org//j.engappai..
 Sowa JF, Eileen C. Way Implementing a Semantic Interpreter Using Conceptual Graphs. IBM Journal of Research and Development    .
 Steinberger, J.  Poesio, M.  Kabadjov, M.A.  Ježek, K. Two uses of anaphora resolution in summarization. Information Processing & Management, Volume  Issue  November  Pages .
 Sun, J.; Zhang, M.; and Tan, C. Exploring syntactic structural features for subtree alignment using bilingual tree kernels. In Proceedings of ACL, , 
 Trias i Mansilla, A., JL de la Rosa i Esteva. Asknext An Agent Protocol for Social Search. Information Sciences  . 
 Velásquez, Juan D., Luis E. Dujovne, Gaston L’Huillier. Extracting significant Website Key Objects A Semantic Web mining approach. Engineering Applications of Artificial Intelligence, Volume  Issue  December  pp . http//dx.doi.org//j.engappai..
 Vicient, Carlos, David Sánchez, Antonio Moreno. An automatic approach for ontologybased feature extraction from heterogeneous textual resources. Engineering Applications of Artificial Intelligence, Volume  Issue  March  Pages . http//dx.doi.org//j.engappai..
 Vismara, P. and  B. Valery.  Finding Maximum Common  Connected Subgraphs Using Clique Detection or Constraint Satisfaction Algorithms. Modelling, Computation and  Optimization in Information Systems  and  Management Sciences, pages , 
 Wu, Jiangning , Zhaoguo Xuan and Donghua Pan, Enhancing text representation for classification tasks with semantic graph structures, International Journal of Innovative Computing, Information and Control ICIC, Volume  Number B.
 Xiong, Hao and Haitao Mi and Yang Liu and Qun Liu. ForestBased Semantic Role Labeling, in Proc AAAI 
 Xing, Du and Xie Li. Accomodating domainindependence—a new approach to the development of general natural language interfaces. Engineering Applications of Artificial Intelligence, Volume  Issue  March  Pages . 
 Yan, X., Han, J. gSpan GraphBased Substructure Pattern Mining. In Proc. IEEE Int. Conf. on Data Mining, ICDM’ IEEE Computer Society  .
 Yang, ShihYao and VonWun Soo. Extract conceptual graphs from plain texts in patent claims.Engineering Applications of Artificial Intelligence, Volume  Issue  June  Pages . http//dx.doi.org//j.engappai..
 Zhang, M.; Che, W.; Zhou, G.; Aw, A.; Tan, C.; Liu, T.; and Li, S.  Semantic role labeling using a grammardriven convolution tree kernel. IEEE transactions on audio, speech, and language processing .
H

